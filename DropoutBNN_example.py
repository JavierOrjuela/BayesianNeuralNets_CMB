""""
Bayesian neural network based on Dropout(Gal et.al., 2015).
Code for extracting the cosmological CMB parameters.
architecture implemented in sonnet: https://sonnet.readthedocs.io/en/latest/
Main Author:
Hector Javier Hortua
orjuelajavier@gmail.com

""""


import json
import pandas as pd
import numpy as np
import tensorflow as tf
import os
import sonnet as snt
from collections import defaultdict
import tensorflow_probability as tfp
import math
import time
import matplotlib
matplotlib.use('Agg')
from matplotlib import pyplot as plt
from matplotlib.patches import Ellipse
import matplotlib.patches as patches
import warnings
import matplotlib.cbook
from scipy.stats import chi2
from concrete_dropout import concrete_dropout
os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"]="2"


""""
Load dataset generated by CMB data generator.
Dataset provided in Pandas Dataframe
""""
image_File = '/ssd_data/CMB/'
json_filename =image_File+  "/params.json"
list_params=['omega_b','omega_cdm','A_s']
json_filename =image_File+  "/params.json"
csv_filename = image_File + '/labels_file.csv'
label_df = pd.read_csv(csv_filename, sep="\t")

def compute_min_max_data():
    all_max = []
    all_min = []
    csv_filename = image_File + '/labels_file.csv'
    label_df = pd.read_csv(csv_filename, sep="\t")
    filenames = label_df['filename']

    for filename in filenames:
        patch1 = np.load(image_File + "/" + filename)

        all_min.append(np.min(patch1))
        all_max.append(np.max(patch1))

    data_min = np.min(all_min)
    data_max = np.max(all_max)
    return data_min, data_max

data_min, data_max = compute_min_max_data()

labels_norm = label_df[list_params].values.astype(np.float32)
max_train = np.max(labels_norm,axis=0)
min_train = np.min(labels_norm,axis=0)


csv_filename_Train = image_File + '/Train_CMB_data.csv'
label_df_Train = pd.read_csv(csv_filename_Train, sep="\t")

csv_filename_Test = image_File + '/Test_CMB_data.csv'
label_df_Test = pd.read_csv(csv_filename_Test, sep="\t")

csv_filename_Validation = image_File + '/Validation_CMB_data.csv'
label_df_Validation = pd.read_csv(csv_filename_Validation, sep="\t")


shuffle_data_Train= label_df_Train.sample(frac=1)
label_df_Train=shuffle_data_Train

shuffle_data_Test= label_df_Test.sample(frac=1)
label_df_Test=shuffle_data_Test

shuffle_data_Validation= label_df_Validation.sample(frac=1)
label_df_Validation=shuffle_data_Validation

label_df_Test = pd.concat([label_df_Test, label_df_Validation], ignore_index=True)


#train
filenames_training = label_df_Train['filename'].values

labels_training = label_df_Train[list_params].values.astype(np.float32)


labels_training = 2*(labels_training-min_train)/(max_train-min_train)-1


#testing
filenames_testing = label_df_Test['filename'].values
labels_testing = label_df_Test[list_params].values.astype(np.float32)

labels_testing =  2*(labels_testing-min_train)/(max_train-min_train)-1

size_image = np.load(image_File+'/'+filenames_testing[0]).shape[1]
size_label = labels_testing.shape[1]

#Validation
filenames_validation = label_df_Validation['filename'].values
labels_validation = label_df_Validation[list_params].values.astype(np.float32)

labels_validation =  2*(labels_validation-min_train)/(max_train-min_train)-1

size_label_Validation = labels_validation.shape[1]


dim_train=len(labels_training)
dim_test=len(labels_testing)
dim_validation=len(labels_validation)


def parse_function(filename, label):
    image = np.load(image_File+'/'+filename.decode())
    image = 2*(image-data_min)/(data_max-data_min)-1
    image = np.expand_dims(image, axis=-1)

    return image.astype(np.float32), label

batch_size = 32

#Training
dataset_training = tf.data.Dataset.from_tensor_slices((filenames_training, labels_training))
dataset_training = dataset_training.shuffle(len(filenames_training))
dataset_training = dataset_training.map(
    lambda filename, label: tuple(
    tf.py_func(
        parse_function, [filename, label], [tf.float32, label.dtype])
    ), num_parallel_calls=4)

#dataset = dataset.map(train_preprocess, num_parallel_calls=4)
dataset_training = dataset_training.batch(batch_size)
dataset_training = dataset_training.prefetch(1)

iterator_training = dataset_training.make_initializable_iterator()
X_image_train,Y_labels_train = iterator_training.get_next()
X_image_train.set_shape([None,size_image,size_image,1])
Y_labels_train.set_shape([None,size_label])
init_op_training = iterator_training.initializer


#Testing
dataset_testing = tf.data.Dataset.from_tensor_slices((filenames_testing, labels_testing))
dataset_testing = dataset_testing.shuffle(len(filenames_testing))
dataset_testing = dataset_testing.map(
     lambda filename, label: tuple(
     tf.py_func(
         parse_function, [filename, label], [tf.float32, label.dtype])
     )
    , num_parallel_calls=4)
#dataset = dataset.map(train_preprocess, num_parallel_calls=4)
dataset_testing = dataset_testing.batch(batch_size)
dataset_testing = dataset_testing.prefetch(1)

iterator_testing = dataset_testing.make_initializable_iterator()
X_image_test,Y_labels_test = iterator_testing.get_next()
X_image_test.set_shape([None,size_image,size_image,1])
Y_labels_test.set_shape([None,size_label])
init_op_testing = iterator_testing.initializer


#Validation
dataset_validation = tf.data.Dataset.from_tensor_slices((filenames_validation, labels_validation))
dataset_validation = dataset_validation.shuffle(len(filenames_validation))
dataset_validation = dataset_validation.map(
     lambda filename, label: tuple(
     tf.py_func(
         parse_function, [filename, label], [tf.float32, label.dtype])
     )
    , num_parallel_calls=4)
#dataset = dataset.map(train_preprocess, num_parallel_calls=4)
dataset_validation = dataset_validation.batch(batch_size)
dataset_validation = dataset_validation.prefetch(1)

iterator_validation = dataset_validation.make_initializable_iterator()
X_image_validation,Y_labels_validation = iterator_validation.get_next()
X_image_validation.set_shape([None,size_image,size_image,1])
Y_labels_validation.set_shape([None,size_label])
init_op_validation = iterator_validation.initializer

tf.reset_default_graph

"""""
Architecture, dropout is applied
after each layer
""""
class Network2(snt.AbstractModule):

  def __init__(self, num_classes,drop_out_rate, name="Network2"):
    super(Network2, self).__init__(name=name)
    self._num_classes = num_classes
    self._output_channels = [16, 16, 32, 32, 32]
    self._num_layers = len(self._output_channels)

    self._kernel_shapes = [[3,3]]*(self._num_layers)
    self._strides = [4,1,1,1,1]
    self._paddings = [snt.VALID]+[snt.SAME]*(self._num_layers-1)
    self._rate = drop_out_rate
  def _build(self, inputs, is_training=None):

    net = inputs
    scale_w=1e-3
    scale_b=1e-4
    initializer={'w':tf.contrib.layers.xavier_initializer(),/
                'b':tf.constant_initializer(10)}
    regularizers = {'w':tf.contrib.layers.l2_regularizer(scale_w),/
     'b':tf.contrib.layers.l2_regularizer(scale_b)}

    layers = [snt.Conv2D(name="conv_2d_{}".format(i),
                         output_channels=self._output_channels[i],
                         kernel_shape=self._kernel_shapes[i],
                         stride=self._strides[i],
                         padding=self._paddings[i],
                         initializers=initializer,
                         regularizers = regularizers,
                         use_bias=True) for i in range(self._num_layers)]


    for i, layer in enumerate(layers):
        net = layer(net)
        net = tf.layers.dropout(net, self._rate, training=is_training)
        bn = snt.BatchNorm(name="batch_norm".format(i))
        net = bn(net, is_training=is_training, test_local_stats=test_local_stats)
        net = tf.nn.relu(net)
        if i==0 or i==1 or i==4:
            net = tf.layers.average_pooling2d(net,pool_size=2,strides=2,padding="SAME")
    net=snt.Conv2D(name="conv_2dl_1",
                         output_channels=256,
                         kernel_shape=[2,2],
                         stride=1,
                         padding='VALID',
                         initializers=initializer,
                         regularizers = regularizers,
                         use_bias=True)(net)
    net = tf.layers.dropout(net, self._rate, training=is_training)
    bn = snt.BatchNorm(name="batch_norm_ff")
    net = bn(net, is_training=is_training, test_local_stats=test_local_stats)
    net = tf.nn.relu(net)
    net = snt.BatchFlatten(name="flatten")(net)
    logits = snt.Linear(self._num_classes)(net)
    return logits


_drop_out_rate=0.1
size_label=len(list_params)
size_nn= int(size_label+ size_label*(size_label+1)/2)
model_1 = Network2(num_classes=size_nn,drop_out_rate= _drop_out_rate)
Y_predicted = model_1(X_image_train, is_training=True)
test_predictions = model_1(X_image_test, is_training=False)
tolerance=10e-4

def make_dist(prediction_nn,z_size =size_label):
    """"
    The network's output are located in the covariance matrix
    semipositive definite

    """"
    net = prediction_nn
    covariance_weights= net[:, z_size:]
    lower_triangle = tf.contrib.distributions.fill_triangular(covariance_weights)
    diag = tf.matrix_diag_part(lower_triangle)
    diag_positive = tf.nn.softplus(diag)+tolerance
    covariance_matrix = lower_triangle - tf.matrix_diag(diag) + tf.matrix_diag(diag_positive)

    distri=tfp.distributions.MultivariateNormalTriL(
        loc=net[:,:z_size], scale_tril=covariance_matrix)
    distri_mean = distri.mean()
    distri_covariance =distri.covariance()
    distri_variance =distri.variance()
    return distri,distri_mean,distri_variance,distri_covariance

"""""
minimize negative_log_likelihood
"""""

step = tf.placeholder(tf.int32)
learning_rate = 10e-6 + tf.train.exponential_decay(10e-4, step, 250, 1/math.e)

Distr_train,Distr_mean_train,Distr_var_train,Distr_covar_train = make_dist(Y_predicted)
Distr_test,Distr_mean_test,Distr_var_test,Distr_covar_test   = make_dist(test_predictions)
negative_log_likelihood_train =  -tf.reduce_mean(Distr_train.log_prob(Y_labels_train))
update_ops = tf.group(*tf.get_collection(tf.GraphKeys.UPDATE_OPS))
reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
negative_log_likelihood_test =  -tf.reduce_mean(Distr_test.log_prob(Y_labels_test))

train_op  = tf.train.AdamOptimizer(learning_rate).minimize(negative_log_likelihood_train
                                                           +tf.reduce_sum(reg_losses))
training_baseline_op = tf.group([train_op , update_ops])


X_feeded_image_test = tf.placeholder(tf.float32, shape=[None,size_image,size_image,1])
def MC_sampling(x):
    return model_1(x, is_training=False)
Distr_test_feed, Distr_mean_test_feed, Distr_var_test_feed,Distr_covar_test_feed = make_dist(MC_sampling(X_feeded_image_test))
Distr_test_feed_sample = Distr_test_feed.sample()
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
init = tf.global_variables_initializer()

Directory_plot_name='Plots_Bayes/'
suffix_='BNNDropout_{}'.format(_drop_out_rate)
Directory_data_name='data_Bayes/'
plots_name=Directory_plot_name+suffix_
data_name=Directory_data_name+suffix_

def coef_deter(predic,real):
    mean=np.mean(real,axis=0)
    numerator=np.sum(np.square(predic -real),axis=0)
    denominator=np.sum(np.square(real -mean),axis=0)
    return 1.-(numerator/denominator)

def unnormalized(valu):
        unormal= valu*(max_train-min_train)+min_train
        num = 0.0002
        scalar = 1e8
        b = tuple({i for i in range(unormal.shape[1]) if i > num})
        unormal[:, b] *= scalar
        return unormal

def CI(predictions, rea_valu, variance_s, covariance_s):
    """"
    Function used for computing the coverage probability
    for confidence invervals 1,2,3 \sigma
    """"

    batch_size = rea_valu.shape[0]
    mean_pred =  np.mean(predictions, axis=2)
    var_pred = np.var(predictions, axis=2)
    cov_pred = np.array(list(map(lambda x: np.cov(x), predictions)))
    mean_var = np.mean(variance_s, axis=2)
    mean_covar = np.mean(covariance_s, axis=3)
    total_variance = var_pred + mean_var
    total_covariance = cov_pred + mean_covar
    total_std = np.sqrt(total_variance)
    summa_68_95_99 = general_ellip_counts(total_covariance, mean_pred, rea_valu)

    return summa_68_95_99, batch_size,\
           total_std, cov_pred, mean_covar, total_covariance, rea_valu, mean_pred

def general_ellip_counts(covariance, mean, real_values):
    Inverse_covariance = np.linalg.inv(covariance)
    Ellip_eq = np.einsum('nl,nlm,mn->n', (real_values - mean), Inverse_covariance, (real_values - mean).T)
    ppf_run=[0.68,0.955,0.997]
    summa_T=[0,0,0]
    rv = chi2(df=mean.shape[1])
    for ix, ppf in enumerate(ppf_run):
        square_norm = rv.ppf(ppf)
        values = Ellip_eq / square_norm
        for ids, inst in enumerate(values):
            if inst <= 1:
                summa_T[ix] += 1
            else:
                pass
    return summa_T



num_batches = dim_train//batch_size
num_batches_test = dim_test//batch_size
count_posterior =2500
epochs=100
with tf.Session(config=config) as session:
    session.run(init)
    loss=[]
    num_elements =[]
    num_elements1 =[]

    for i in range(1,epochs):
        session.run(init_op_training)
        while True:
             try:
                 _, loss_val, predic_val_train, rea_valu_train,std_train, lr = session.run([training_baseline_op,
                                            negative_log_likelihood_train,
                                           Distr_mean_train, Y_labels_train, Distr_var_train,
                                            learning_rate],feed_dict={step:i})
                 val_coef=coef_deter(predic_val_train, rea_valu_train)
             except tf.errors.OutOfRangeError:
                    print('train_log: {}  R^2: {} Epoch: {}'.format(loss_val ,val_coef, i))

                    break
        session.run(init_op_testing)

        list_conf_68=0
        list_conf_95=0
        list_conf_99=0
        list_conf_T =0
        list_conf_68_95_99=[]

        loss_val_tests = session.run(negative_log_likelihood_test )
        with open(data_name+'loss.dat', 'a') as ft:
                ft.write("{} {} {} \n".format(i, loss_val, loss_val_tests))
        ft.close()
        number_test_images_running=100
        MCdrop=60
        if  i%MCdrop==0:
            for number_test_images in range(number_test_images_running):
                loss_val_test,predic_val,rea_valu,std,X_image_feed = session.run([negative_log_likelihood_test,
                                      Distr_mean_test, Y_labels_test, Distr_var_test,X_image_test
                                                                                ])

                testpred_feeded_total =[]
                testpred_feeded_total_var=[]
                testpred_feeded_total_covar=[]
                testpred_feeded_total_sample=[]

                for _ in range(count_posterior):

                    means_feed, var_feed,covar_feed,sample_feed = session.run([Distr_mean_test_feed,
                                                                            Distr_var_test_feed,
                                                                            Distr_covar_test_feed,
                                                                            Distr_test_feed_sample],
                                                                            feed_dict={X_feeded_image_test:X_image_feed})


                    testpred_feeded_total.append(means_feed)
                    testpred_feeded_total_var.append(var_feed)
                    testpred_feeded_total_covar.append(covar_feed)
                    testpred_feeded_total_sample.append(sample_feed)

                testpred_feeded_stack = np.stack(testpred_feeded_total, axis=2)
                testpred_feeded_stack_var = np.stack(testpred_feeded_total_var, axis=2)
                testpred_feeded_stack_sample=np.stack(testpred_feeded_total_sample, axis=2)
                testpred_feeded_stack_covar = np.stack(testpred_feeded_total_covar, axis=3)
                coverage_v_68_95_99,coverage_T,\
                Total_std, cov_of_predictions, means_of_covar, Total_covariance, rea_valu, mean_pred = CI(
                                                                   testpred_feeded_stack,
                                                                   rea_valu,
                                                                   testpred_feeded_stack_var,
                                                                   testpred_feeded_stack_covar)

                list_conf_68 += coverage_v_68_95_99[0]
                list_conf_95 += coverage_v_68_95_99[1]
                list_conf_99 += coverage_v_68_95_99[2]
                list_conf_T  += coverage_T

            list_conf_68_95_99=[list_conf_68,list_conf_95,list_conf_99]

            np.save(data_name+'means_means_{}_{}'.format(number_test_images,i) , mean_pred)
            np.save(data_name+'covs_means_{}_{}'.format(number_test_images,i) , means_of_covar)
            np.save(data_name+'means_covs_{}_{}'.format(number_test_images,i) , cov_of_predictions)
            np.save(data_name+'chains_{}_{}'.format(number_test_images,i) , testpred_feeded_stack_sample)
            np.save(data_name+'real_value_{}_{}'.format(number_test_images,i) , rea_valu)
            np.save(data_name+'Total_std_{}_{}'.format(number_test_images,i) , Total_std)

            print('conf.68_95_99: {}    \
                   count_T: {}'.format(list_conf_68_95_99,list_conf_T))
            print('test_log: {}  R^2:{} Epoch: {}'.format(loss_val_test,
                                                           coef_deter(predic_val,rea_valu),
                                                           i))
            fig_1 = plt.figure()
            colors=['blue','red','green','orange', 'darkviolet','purple']
            ecolors=['lightblue','lightcoral','lightgreen','yellow', 'violet','magenta']
            for m in range(len(list_params)):
                plt.errorbar(rea_valu[:,m], mean_pred[:,m], Total_std[:,m],fmt='o', color=colors[m],
                                     ecolor=ecolors[m], elinewidth=3, capsize=0, label=list_params[m])

                line_s1=np.arange(-0.01,1,0.01)
                plt.plot(line_s1,line_s1,'r-', alpha=0.1)
                plt.xlabel('True value')
                plt.ylabel('Predicted value')
                plt.legend()

            plt.savefig(plots_name+"plot_parameters_{}_{}.png".format(i,number_test_images))
            plt.close(fig_1)
            with open(data_name+'info_covs_{}.dat'.format(i), 'a') as ft1:
                  ft1.write("{} {} {} {} {} {}\n".format(list_conf_68_95_99, list_conf_T,
                                                   np.array(list_conf_68_95_99)/list_conf_T,max_train,min_train))
            ft1.close()
